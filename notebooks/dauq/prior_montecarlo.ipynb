{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and process the prior monte carlo and pick a \"truth\" realization\n",
    "\n",
    "A great advantage of exploring a synthetic model is that we can enforce a \"truth\" and then evaluate how our various attempts to estimate it perform. One way to do this is to run a monte carlo ensemble of multiple parameter realizations and then choose one of them to represent the \"truth\". That will be accomplished in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.rcParams['font.size']=12\n",
    "import flopy\n",
    "import pyemu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPER IMPORTANT: SET HOW MANY PARALLEL WORKERS TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the `t_d` or \"template directory\" variable to point at the template folder and read in the PEST control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"template\"\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.npar_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the previously generated parameter ensemble and inspect (again!)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pyemu.ParameterEnsemble.from_binary(pst=pst,filename=os.path.join(t_d,\"prior.jcb\"))\n",
    "#pe.loc[:,should_fix] = 1.0\n",
    "pe.to_csv(os.path.join(t_d,\"sweep_in.csv\"))\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.loc[:,\"hk031\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.loc[:,\"hk031\"].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look! hk is log-normal-ish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the first realization through the pest interface for a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the par vals with the first row in the par ensemble\n",
    "pst.parameter_data.loc[pe.columns,\"parval1\"] = pe.iloc[0,:]\n",
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(t_d,\"test.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-ies test.pst\",cwd=t_d)\n",
    "res = pyemu.pst_utils.read_resfile(os.path.join(t_d,\"test.base.rei\"))\n",
    "res.loc[pst.nnz_obs_names,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the prior ensemble in parallel locally\n",
    "This takes advantage of the program `pestpp-swp` which runs a parameter sweep through a set of parameters. By default, `pestpp-swp` reads in the ensemble from a file called `sweep_in.csv` which in this case we made just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = \"master_prior_sweep\"\n",
    "pyemu.os_utils.start_slaves(t_d,\"pestpp-swp\",\"freyberg.pst\",num_slaves=num_workers,slave_root=\".\",master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the output ensemble and plot a few things\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(m_d,\"sweep_out.csv\"),index_col=0)\n",
    "print('number of realization in the ensemble before dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop any failed runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = obs_df.loc[obs_df.failed_flag==0,:]\n",
    "print('number of realization in the ensemble **after** dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confirm which quantities were identified as forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = pst.pestpp_options[\"forecasts\"].split(',')\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we can plot the distributions of each forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in fnames:\n",
    "    plt.figure()\n",
    "    ax = obs_df.loc[:,forecast].plot(kind=\"hist\")\n",
    "    ax.set_title(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that under scenario conditions, many more realizations for the flow to the aquifer in the headwaters are postive (as expected).  Lets difference these two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfnames = [f for f in fnames if \"1980\" in f or \"_001\" in f]\n",
    "hfnames = [f for f in fnames if \"1979\" in f or \"_000\" in f]\n",
    "diff = obs_df.loc[:,hfnames].values - obs_df.loc[:,sfnames].values\n",
    "diff = pd.DataFrame(diff,columns=sfnames)\n",
    "diff.hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the most extreme scenario yields a large decrease in flow from the aquifer to the headwaters (the most negative value).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many modeling analyses could stop right here to avoid the ill-effects of history matching..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting the \"truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to replace the observed values (`obsval`) in the control file with the outputs for one of the realizations in `obs_df` that we consider to be the ``truth``.  In this way, we now have the nonzero values for history matching, but also the ``truth`` values for comparing how we are doing with other unobserved quantities.  I'm going to pick a realization that yields an \"average\" variability of the observed gw levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vals = obs_df.loc[:,\"fa_tw_19791230\"].sort_values()\n",
    "sorted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sorted_vals.index[100]\n",
    "idx  # candidate truth realization index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the outputs corresponding to available observations and forecasts for this realization look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.loc[idx,pst.nnz_obs_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our selected truth does with the sw/gw forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.loc[idx,fnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights!!!\n",
    "Assign some initial weights. Now, it is custom to add noise to the observed values...we will use the classic Gaussian noise...zero mean and standard deviation of 1 over the weight (which we will now specify).  We will speak more about noise and its sources shortly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))\n",
    "obs = pst.observation_data\n",
    "obs.loc[:,\"obsval\"] = obs_df.loc[idx,pst.obs_names]\n",
    "obs.loc[obs.obgnme==\"calhead\",\"weight\"] = 5.0  # this corresponds to an (expected) noise standard deviation of 20 cm...\n",
    "obs.loc[obs.obgnme==\"calflux\",\"weight\"] = 0.01  # corresponding to an (expected) noise standard deviation of 100 m^3/d...\n",
    "obs.loc[pst.nnz_obs_names,\"weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we just get a sample from a random normal distribution with mean=0 and std=1.\n",
    "The argument indicates how many samples we want - and we choose `pst.nnz_obs` which is the \n",
    "the number of nonzero-weighted observations in the PST file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "snd = np.random.randn(pst.nnz_obs)\n",
    "noise = snd * 1./obs.loc[pst.nnz_obs_names,\"weight\"]\n",
    "pst.observation_data.loc[noise.index,\"obsval\"] += noise\n",
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write this out to a new file and run `pestpp-ies` to see how the objective function looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,\"freyberg.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in the results and make some figures showing residuals and the balance of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))\n",
    "print(pst.phi)\n",
    "plt.figure()\n",
    "pst.plot(kind='phi_pie');\n",
    "print('Here are the non-zero weighted observation contributions to phi')\n",
    "\n",
    "figs = pst.plot(kind=\"1to1\");\n",
    "pst.res.loc[pst.nnz_obs_names,:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the \"truth\" model once and inspect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pd.read_csv(os.path.join(m_d,\"sweep_in.csv\"),index_col=0)\n",
    "pst.parameter_data.loc[:,\"parval1\"] = par_df.loc[idx,pst.par_names]\n",
    "pst.write(os.path.join(m_d,\"test.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will run this with `noptmax=0` to perform a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies.exe test.pst\",cwd=m_d)\n",
    "pst = pyemu.Pst(os.path.join(m_d,\"test.pst\"))\n",
    "print(pst.phi)\n",
    "pst.res.loc[pst.nnz_obs_names,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual should be exactly the noise values from above. Lets load the model (that was just run using the true pars) and check some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = m.rch.rech[0].array\n",
    "#a = m.rch.rech[0].array\n",
    "a = np.ma.masked_where(m.bas6.ibound[0].array==0,a)\n",
    "print(a.min(),a.max())\n",
    "c = plt.imshow(a)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = flopy.utils.MfListBudget(os.path.join(m_d,\"freyberg.list\"))\n",
    "df = lst.get_dataframes(diff=True)[0]\n",
    "ax = df.plot(kind=\"bar\",figsize=(10,10), grid=True)\n",
    "a = ax.set_xticklabels([\"historic\",\"scenario\"],rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see how our existing observation ensemble compares to the truth\n",
    "\n",
    "forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "plt.figure()\n",
    "for forecast in fnames:\n",
    "    ax = plt.subplot(111)\n",
    "    obs_df.loc[:,forecast].hist(ax=ax,color=\"0.5\",alpha=0.5)\n",
    "    ax.plot([obs.loc[forecast,\"obsval\"],obs.loc[forecast,\"obsval\"]],ax.get_ylim(),\"r\")\n",
    "    ax.set_title(forecast)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for oname in pst.nnz_obs_names:\n",
    "    ax = plt.subplot(111)\n",
    "    obs_df.loc[:,oname].hist(ax=ax,color=\"0.5\",alpha=0.5)\n",
    "    ax.plot([obs.loc[oname,\"obsval\"],obs.loc[oname,\"obsval\"]],ax.get_ylim(),\"r\")\n",
    "    ax.set_title(oname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
